{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "# warnings.filterwarnings('ignore')      # never show warnings _> BIG CHANGE\n",
    "warnings.filterwarnings(action='once') # show warnings once and never again\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import optimizers\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Dropout, LSTM, CuDNNLSTM\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import pyarrow.parquet as pyparquet\n",
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import math\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score, f1_score, confusion_matrix\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# save and load tokenizer\n",
    "import pickle \n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Importing the graph_objs module which contains plotting objects\n",
    "import plotly.graph_objs as go\n",
    "from IPython.core.interactiveshell import InteractiveShell \n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "from plotly.offline import iplot\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find threshold for precision \n",
    "with open(\"docker_save_data.txt\",\"w\") as file_thresholds:\n",
    "    precisions_thresholds = [0.8, 0.85, 0.9, 0.95] \n",
    "    for p in precisions_thresholds: \n",
    "        t = 3333\n",
    "        msg = \"Threshold with precision > %.2f%%: %.2f%%\" % (p, t)\n",
    "        print(msg)\n",
    "        file_thresholds.write(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "\n",
    "embedding_vecor_length = 300\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posfile = 'data/qc-deletionreason-contactrequest.snappy.parquet'\n",
    "negfile = 'data/qc-notdeleted.snappy.parquet'\n",
    "\n",
    "pos = pyparquet.read_table(posfile).to_pandas()[[\"label\", \"decoded_title\", \"decoded_body\"]]\n",
    "neg = pyparquet.read_table(negfile).to_pandas()[[\"label\", \"decoded_title\", \"decoded_body\"]]\n",
    "\n",
    "print(\"Pos dataset: {}\".format(len(pos)))\n",
    "print(\"Neg dataset: {}\".format(len(neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the size of neg and pos to N\n",
    "N = 10000.0\n",
    "pos = pos.sample(frac=N/len(pos)).reset_index(drop=True)\n",
    "neg = neg.sample(frac=N/len(neg)).reset_index(drop=True)\n",
    "\n",
    "print(\"Pos dataset: {}\".format(len(pos)))\n",
    "print(\"Neg dataset: {}\".format(len(neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_train, neg_test = train_test_split(neg, test_size=0.2)\n",
    "pos_train, pos_test = train_test_split(pos, test_size=0.2)\n",
    "\n",
    "# # upsample legit questions in training dataset\n",
    "# legit_train = pd.concat([legit_train]*2, ignore_index=True) # Ignores the index\n",
    "\n",
    "print(\"Training dataset pos: {}\".format(len(pos_train)))\n",
    "print(\"Testing dataset pos: {}\".format(len(pos_test)))\n",
    "print(\"Training dataset neg: {}\".format(len(neg_train)))\n",
    "print(\"Testing dataset neg: {}\".format(len(neg_test)))\n",
    "\n",
    "# concatinate adult and legit and shuffle the resulting test and train sets\n",
    "train = pd.concat([pos_train, neg_train]).sample(frac=1).reset_index(drop=True)\n",
    "test = pd.concat([pos_test, neg_test]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(\"Training dataset: {}\".format(len(train)))\n",
    "print(\"Testing dataset: {}\".format(len(test)))\n",
    "\n",
    "print(\"Pos dataset: {}\".format(len(pos)))\n",
    "print(\"Neg dataset: {}\".format(len(neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['question_text'] = train['decoded_title'].map(str) + \" \" + train['decoded_body'].map(str)\n",
    "test['question_text'] = test['decoded_title'].map(str) + \" \" + test['decoded_body'].map(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.question_text.str.split().str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.question_text.str.split().str.len().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- set maxlen parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fill up the missing values\n",
    "train_X = train[\"question_text\"].fillna(\"_na_\").values\n",
    "test_X = test[\"question_text\"].fillna(\"_na_\").values\n",
    "\n",
    "## Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=max_features, filters='', lower=False)\n",
    "tokenizer.fit_on_texts(list(train_X))\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "## Pad the sentences \n",
    "train_X = pad_sequences(train_X, maxlen=maxlen, padding='post', truncating='post')\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "train[\"target\"] = [\"0\" if x ==\"__label__legit\" else \"1\" for x in train[\"label\"]]\n",
    "test[\"target\"] = [\"0\" if x ==\"__label__legit\" else \"1\" for x in test[\"label\"]]\n",
    "\n",
    "test_y = test['target']\n",
    "train_y = train['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment it because of lower word coverage (v4)\n",
    "# EMBEDDING_FILE = '../input/gf-embeddings-v2/gfmodel_v6.vec' --> memory error\n",
    "EMBEDDING_FILE = '../data/gfmodel_v4.vec'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(EMBEDDING_FILE)\n",
    "for line in tqdm(f):\n",
    "    values = line.split(\" \")\n",
    "    if len(values) == 302:\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:301], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## embedding setup\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://)- number of memory cells set to less than 80-400 based on ([link](https://ai.stackexchange.com/questions/3156/how-to-select-number-of-hidden-layers-and-number-of-memory-cells-in-an-lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def model_init():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embedding_vecor_length, input_length=maxlen,  weights=[embedding_matrix]))\n",
    "    model.add(Dropout(0.8))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.8))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_init()\n",
    "tensorboard_callback = TensorBoard(\"logs\")\n",
    "history = model.fit(train_X, train_y, epochs=10, batch_size=64, validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001), tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['acc'], label='train')\n",
    "plt.plot(history.history['val_acc'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots suggest that the model has a little over fitting problem, more data may help, but more epochs will not help using the current data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the extension and start TensorBoard\n",
    "\n",
    "# %load_ext tensorboard.notebook\n",
    "# %tensorboard --logdir logs\n",
    "# %reload_ext tensorboard.notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 3\n",
    "\n",
    "model = model_init()\n",
    "\n",
    "history = model.fit(train_X, train_y, epochs=epoch_num, batch_size=64, validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001), tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(test_X, test_y, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(test_X)\n",
    "labels = [1 if x == \"1\" else 0 for x in test_y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall = precision_recall_curve(labels, prediction)\n",
    "precision_recall = pd.DataFrame.from_records(precision_recall).T\n",
    "precision_recall.columns = ['Precision', 'Recall', 'Thresholds']\n",
    "precision_recall[\"f1_score\"] = 2 * (precision_recall.Precision * precision_recall.Recall) / (precision_recall.Precision + precision_recall.Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model by max. f1-score\n",
    "print(\"Max f1-Score: %.2f%%\" % (max(precision_recall[\"f1_score\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find threshold for precision \n",
    "with open(\"qc_contact_request_deletion_reason_thresholds.txt\",\"w\") as file_thresholds:\n",
    "    precisions_thresholds = [0.8, 0.85, 0.9, 0.95] \n",
    "    for p in precisions_thresholds: \n",
    "        t = min(precision_recall[precision_recall['Precision']>p][\"Thresholds\"])\n",
    "        msg = \"Threshold with precision > %.2f%%: %.2f%%\" % (p, t)\n",
    "        print(msg)\n",
    "        file_thresholds.write(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print precision, recall, f1-score curve\n",
    "# # precision_recall.iplot(x='Thresholds', xTitle='Decision Threshold',yTitle='Score', title='Precision/Recall Tradeoff')\n",
    "# precision_recall.plot(x='Thresholds')\n",
    "\n",
    "# Trace1 can be viewed like a geom_point() layer with various arguements\n",
    "trace1 = go.Scatter(x=precision_recall.Thresholds, y = precision_recall.Precision,  marker=dict(size=5,\n",
    "                line=dict(width=1),\n",
    "                color=\"blue\"\n",
    "               ), \n",
    "                    mode=\"lines\", name='Precision')\n",
    "trace2 = go.Scatter(x=precision_recall.Thresholds, y = precision_recall.Recall,  marker=dict(size=5,\n",
    "                line=dict(width=1),\n",
    "                color=\"orange\"\n",
    "               ), \n",
    "                    mode=\"lines\", name='Recall')\n",
    "\n",
    "trace3 = go.Scatter(x=precision_recall.Thresholds, y = precision_recall.f1_score,  marker=dict(size=5,\n",
    "                line=dict(width=1),\n",
    "                color=\"green\"\n",
    "               ), \n",
    "                    mode=\"lines\", name='f1_score')\n",
    "\n",
    "\n",
    "\n",
    "data1 = go.Data([trace1, trace2, trace3])\n",
    "layout1=go.Layout(title=\"Threshold vs P, R, F1\", xaxis={'title':'Threshold'}, yaxis={'title':'Threshold Tradeoff'})\n",
    "figure1=go.Figure(data=data1,layout=layout1)\n",
    "iplot(figure1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predLabel = pd.DataFrame(prediction, columns = [\"Probability\"]).assign(Label=labels)\n",
    "trace4 = go.Histogram(\n",
    "    x=predLabel[predLabel.Label == 0].Probability,\n",
    "    opacity=1,\n",
    "    name = \"1\"\n",
    ")\n",
    "trace5 = go.Histogram(\n",
    "    x=predLabel[predLabel.Label == 1].Probability,\n",
    "    opacity=0.3,\n",
    "    name = \"0\"\n",
    ")\n",
    "\n",
    "data45 = go.Data([trace4, trace5])\n",
    "layout45 = go.Layout(barmode='overlay')\n",
    "figure45 = go.Figure(data=data45, layout=layout45)\n",
    "\n",
    "iplot(figure45, filename='probability overlaid histogram')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.9\n",
    "y_true = [1 if x ==\"1\" else 0 for x in test_y]\n",
    "y_pred = [1 if x > threshold else 0 for x in prediction]\n",
    "confusion_matrix(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionDF = pd.DataFrame({'prediction':prediction[:,0]})\n",
    "predicted_test = test.join(predictionDF, how='inner')\n",
    "predicted_test['y_pred'] = predicted_test['prediction'].progress_apply(lambda x: 1 if x > threshold else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false negatives\n",
    "predicted_test[(predicted_test['y_pred'] == 0) & (predicted_test['target'] == '1')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false positive\n",
    "predicted_test[(predicted_test['y_pred'] == 1) & (predicted_test['target'] == '0')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on sample from \"allquestion\" dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate positive and negative samples\n",
    "data = pd.concat([pos, neg]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data['question_text'] = data['decoded_title'].map(str) + \" \" + data['decoded_body'].map(str)\n",
    "\n",
    "## fill up the missing values\n",
    "data_X = data[\"question_text\"].fillna(\"_na_\").values\n",
    "\n",
    "## Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=max_features, filters='', lower=False)\n",
    "tokenizer.fit_on_texts(list(data_X))\n",
    "data_X = tokenizer.texts_to_sequences(data_X)\n",
    "\n",
    "## Pad the sentences \n",
    "data_X = pad_sequences(data_X, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "data[\"target\"] = [\"0\" if x ==\"__label__legit\" else \"1\" for x in data[\"label\"]]\n",
    "\n",
    "data_y = data['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokenizer for model\n",
    "with open('qc_contact_request_deletion_reason_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('qc_contact_request_deletion_reason_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(tokenizer.to_json())    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_init()\n",
    "\n",
    "model.fit(data_X, data_y, epochs=epoch_num, batch_size=64, validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "\n",
    "# save model\n",
    "modelfile = \"qc_contact_request_deletion_reason_model.h5\"\n",
    "model.save(modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def res_cmd(cmd):\n",
    "  return str(subprocess.Popen(cmd, stdout=subprocess.PIPE,shell=True).communicate()[0])\n",
    "\n",
    "def str_md5sum(abspath_file):\n",
    "    str_res = res_cmd('md5sum \"%s\"' % abspath_file)\n",
    "    return str_res.split(\"'\")[1].replace('\\\\n', '')\n",
    "\n",
    "print(str_md5sum(modelfile))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
