{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "\n",
    "# save and load tokenizer\n",
    "import pickle \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = Path(os.getcwd())\n",
    "data_dir = cwd.parent/'data'\n",
    "resources_dir = cwd.parent/'resources'\n",
    "target_dir = cwd.parent.parent.parent/'target/ml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk(data_dir):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile =data_dir/'questions.csv'\n",
    "data = pd.read_csv(datafile, index_col=0)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "\n",
    "embedding_vecor_length = 300\n",
    "\n",
    "meta_parameters = {\n",
    "    'padding': 'pre',\n",
    "    'truncating':'post'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decide maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.question_text.str.split().str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data balance and upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.1, random_state=42)\n",
    "train, val = train_test_split(train, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Training dataset: {}\".format(len(train)))\n",
    "print(\"Testing dataset: {}\".format(len(test)))\n",
    "print(\"validate dataset: {}\".format(len(val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = len(train[train.target == 0])/len(train[train.target == 1])\n",
    "print(\"Training dataset pos/neg: {}\\n\".format(prep))\n",
    "\n",
    "numNeg = len(train[train.target == 0])\n",
    "pos = train[train.target == 1].sample(numNeg, replace = True, random_state=42).reset_index(drop=True)\n",
    "neg = train[train.target == 0].reset_index(drop=True)\n",
    "train = pd.concat([pos, neg]).reset_index(drop=True)\n",
    "\n",
    "print(\"Training dataset pos: {}\".format(len(train[train.target == 1])))\n",
    "print(\"Training dataset neg: {}\".format(len(train[train.target == 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(question, tokenizer, meta_parameters):\n",
    "\n",
    "    question_X = question[\"question_text\"].fillna(\"_na_\").values\n",
    "    \n",
    "    question_X = tokenizer.texts_to_sequences(question_X)\n",
    " \n",
    "    question_X = pad_sequences(question_X, maxlen=maxlen, padding=meta_parameters['padding'], truncating=meta_parameters['truncating'])\n",
    "    \n",
    "    return question_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizerfile = resources_dir/\"tokenizer.pickle\"\n",
    "with open(tokenizerfile, 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = preprocessing(train, tokenizer, meta_parameters)\n",
    "train_y = train[\"target\"]\n",
    "test_X = preprocessing(test, tokenizer, meta_parameters)\n",
    "test_y = test[\"target\"]\n",
    "val_X = preprocessing(val, tokenizer, meta_parameters)\n",
    "val_y = val[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lzma\n",
    "matrixfile = resources_dir/\"embedding_matrix.pkl.lzma\"\n",
    "with lzma.open(matrixfile, 'rb') as handle:\n",
    "    embedding_matrix = pickle.load(handle)\n",
    "\n",
    "embedding_matrix.shape  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embedding_vecor_length, input_length=maxlen,  weights=[embedding_matrix]))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[metrics.AUC()])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_init()\n",
    "history = model.fit(train_X, train_y, epochs=1, batch_size=64, validation_data=(val_X, val_y)\n",
    "                    , callbacks=[EarlyStopping(monitor='val_loss', patience=1, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(test_X)\n",
    "labels = [x for x in test_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall = precision_recall_curve(labels, prediction)\n",
    "precision_recall = pd.DataFrame.from_records(precision_recall).T\n",
    "precision_recall.columns = ['Precision', 'Recall', 'Thresholds']\n",
    "precision_recall[\"f1_score\"] = 2 * (precision_recall.Precision * precision_recall.Recall) / (precision_recall.Precision + precision_recall.Recall)\n",
    "# Find best model by max. f1-score\n",
    "print(\"Max f1-Score: %.2f%%\" % (max(precision_recall[\"f1_score\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find threshold for precision \n",
    "with open(cwd/\"output/qc_contact_request_deletion_reason_thresholds.txt\",\"w\") as file_thresholds:\n",
    "    precisions_thresholds = [0.8, 0.85, 0.9, 0.95] \n",
    "    for p in precisions_thresholds: \n",
    "        t = min(precision_recall[precision_recall['Precision']>p][\"Thresholds\"])\n",
    "        msg = \"Threshold with precision > %.2f%%: %.2f%%\" % (p, t)\n",
    "        print(msg)\n",
    "        file_thresholds.write(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = go.Scatter(x=precision_recall.Thresholds, y = precision_recall.Precision,  marker=dict(size=5,\n",
    "                line=dict(width=1),\n",
    "                color=\"blue\"\n",
    "               ), \n",
    "                    mode=\"lines\", name='Precision')\n",
    "trace2 = go.Scatter(x=precision_recall.Thresholds, y = precision_recall.Recall,  marker=dict(size=5,\n",
    "                line=dict(width=1),\n",
    "                color=\"orange\"\n",
    "               ), \n",
    "                    mode=\"lines\", name='Recall')\n",
    "\n",
    "trace3 = go.Scatter(x=precision_recall.Thresholds, y = precision_recall.f1_score,  marker=dict(size=5,\n",
    "                line=dict(width=1),\n",
    "                color=\"green\"\n",
    "               ), \n",
    "                    mode=\"lines\", name='f1_score')\n",
    "\n",
    "\n",
    "\n",
    "data1 = [trace1, trace2, trace3]\n",
    "layout1=go.Layout(title=\"Threshold vs P, R, F1\", xaxis={'title':'Threshold'}, yaxis={'title':'Threshold Tradeoff'})\n",
    "figure1=go.Figure(data=data1,layout=layout1)\n",
    "iplot(figure1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predLabel = pd.DataFrame(prediction, columns = [\"Probability\"]).assign(Label=labels)\n",
    "trace4 = go.Histogram(\n",
    "    x=predLabel[predLabel.Label == 0].Probability,\n",
    "    opacity=1,\n",
    "    name = \"1\"\n",
    ")\n",
    "trace5 = go.Histogram(\n",
    "    x=predLabel[predLabel.Label == 1].Probability,\n",
    "    opacity=0.3,\n",
    "    name = \"0\"\n",
    ")\n",
    "\n",
    "data45 = [trace4, trace5]\n",
    "layout45 = go.Layout(barmode='overlay')\n",
    "figure45 = go.Figure(data=data45, layout=layout45)\n",
    "\n",
    "iplot(figure45, filename='probability overlaid histogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.9\n",
    "y_true = [x for x in test_y]\n",
    "y_pred = [1 if x > threshold else 0 for x in prediction]\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionDF = pd.DataFrame({'prediction':prediction[:,0]})\n",
    "predicted_test = test.join(predictionDF, how='inner')\n",
    "predicted_test['y_pred'] = predicted_test['prediction'].progress_apply(lambda x: 1 if x > threshold else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false negatives\n",
    "predicted_test[(predicted_test['y_pred'] == 0) & (predicted_test['target'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false positive\n",
    "predicted_test[(predicted_test['y_pred'] == 1) & (predicted_test['target'] == 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "modelfile = target_dir/\"qc_model.h5\"\n",
    "model.save(modelfile)\n",
    "\n",
    "# save tokenizer for model\n",
    "with open(target_dir/'qc_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(target_dir/'qc_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(tokenizer.to_json())    \n",
    "\n",
    "# save meta parameters\n",
    "with open(target_dir/'qc_meta_parameters.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(meta_parameters, f) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
