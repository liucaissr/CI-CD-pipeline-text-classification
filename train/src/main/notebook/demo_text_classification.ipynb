{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wtf with not validated notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pyarrow.parquet as pyparquet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Dropout, LSTM, CuDNNLSTM\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "\n",
    "# save and load tokenizer\n",
    "import pickle \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = Path(os.getcwd())\n",
    "data_dir = cwd.parent/'data'\n",
    "resources_dir = cwd.parent/'resources'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk(data_dir):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 1 for mask of zero\n",
    "max_features = 10000\n",
    "\n",
    "embedding_vecor_length = 300\n",
    "\n",
    "kind = \"cpu\"\n",
    "\n",
    "meta_parameters = {\n",
    "    'padding': 'pre',\n",
    "    'truncating':'post'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posfile =data_dir/'positive_sample.snappy.parquet'\n",
    "negfile =data_dir/'negative_sample.snappy.parquet'\n",
    "pos = pyparquet.read_table(posfile).to_pandas()[[\"label\", \"decoded_title\", \"decoded_body\"]]\n",
    "neg = pyparquet.read_table(negfile).to_pandas()[[\"label\", \"decoded_title\", \"decoded_body\"]]\n",
    "\n",
    "print(\"Pos dataset: {}\".format(len(pos)))\n",
    "print(\"Neg dataset: {}\".format(len(neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the size of neg and pos to N\n",
    "'''\n",
    "N = 10000.0\n",
    "pos = pos.sample(frac=N/len(pos)).reset_index(drop=True)\n",
    "neg = neg.sample(frac=N/len(neg)).reset_index(drop=True)\n",
    "\n",
    "print(\"Pos dataset: {}\".format(len(pos)))\n",
    "print(\"Neg dataset: {}\".format(len(neg)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_train, neg_test = train_test_split(neg, test_size=0.2, random_state=42)\n",
    "pos_train, pos_test = train_test_split(pos, test_size=0.2, random_state=42)\n",
    "\n",
    "# # upsample legit questions in training dataset\n",
    "# legit_train = pd.concat([legit_train]*2, ignore_index=True) # Ignores the index\n",
    "neg_train, neg_val = train_test_split(neg_train, test_size=0.1, random_state=42)\n",
    "pos_train, pos_val = train_test_split(pos_train, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Training dataset pos: {}\".format(len(pos_train)))\n",
    "print(\"val dataset pos: {}\".format(len(pos_val)))\n",
    "print(\"Testing dataset pos: {}\\n\".format(len(pos_test)))\n",
    "print(\"Training dataset neg: {}\".format(len(neg_train)))\n",
    "print(\"val dataset neg: {}\".format(len(neg_val)))\n",
    "print(\"Testing dataset neg: {}\\n\".format(len(neg_test)))\n",
    "\n",
    "# concatinate adult and legit and shuffle the resulting test and train sets\n",
    "train = pd.concat([pos_train, neg_train]).sample(frac=1).reset_index(drop=True)\n",
    "test = pd.concat([pos_test, neg_test]).sample(frac=1).reset_index(drop=True)\n",
    "val = pd.concat([pos_val, neg_val]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(\"Training dataset: {}\".format(len(train)))\n",
    "print(\"Val dataset: {}\".format(len(val)))\n",
    "print(\"Testing dataset: {}\".format(len(test)))\n",
    "\n",
    "\n",
    "print(\"Pos dataset: {}\".format(len(pos)))\n",
    "print(\"Neg dataset: {}\".format(len(neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['question_text'] = train['decoded_title'].map(str) + \" \" + train['decoded_body'].map(str)\n",
    "test['question_text'] = test['decoded_title'].map(str) + \" \" + test['decoded_body'].map(str)\n",
    "val['question_text'] = val['decoded_title'].map(str) + \" \" + val['decoded_body'].map(str)\n",
    "\n",
    "\n",
    "data = pd.concat([train, test, val]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decide maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.question_text.str.split().str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.question_text.str.split().str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    \n",
    "    data_X = data[\"question_text\"].fillna(\"_na_\").values\n",
    "    \n",
    "    ## Tokenize the sentences\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(data_X))\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(question, tokenizer, meta_parameters):\n",
    "\n",
    "    question_X = question[\"question_text\"].fillna(\"_na_\").values\n",
    "    \n",
    "    question_X = tokenizer.texts_to_sequences(question_X)\n",
    "\n",
    "    ## Pad the sentences \n",
    "    question_X = pad_sequences(question_X, maxlen=maxlen, padding=meta_parameters['padding'], truncating=meta_parameters['truncating'])\n",
    "\n",
    "    question[\"target\"] = [0 if x ==\"__label__legit\" else 1 for x in question[\"label\"]]\n",
    "\n",
    "    question_y = question['target']\n",
    "    \n",
    "    return question_X, question_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenize(data)\n",
    "\n",
    "train_X, train_y = preprocessing(train, tokenizer, meta_parameters)\n",
    "test_X, test_y = preprocessing(test, tokenizer, meta_parameters)\n",
    "val_X, val_y = preprocessing(val, tokenizer, meta_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fill up the missing values\n",
    "train_X = train[\"question_text\"].fillna(\"_na_\").values\n",
    "test_X = test[\"question_text\"].fillna(\"_na_\").values\n",
    "\n",
    "## Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=max_features, filters='', lower=False)\n",
    "tokenizer.fit_on_texts(list(train_X))\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "## Pad the sentences \n",
    "train_X = pad_sequences(train_X, maxlen=maxlen, padding='post', truncating='post')\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "train[\"target\"] = [\"0\" if x ==\"__label__legit\" else \"1\" for x in train[\"label\"]]\n",
    "test[\"target\"] = [\"0\" if x ==\"__label__legit\" else \"1\" for x in test[\"label\"]]\n",
    "\n",
    "test_y = test['target']\n",
    "train_y = train['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # comment it because of lower word coverage (gfmodel_v4)\n",
    "# # EMBEDDING_FILE = '../input/gf-embeddings-v2/gfmodel_v6.vec' --> memory error\n",
    "# EMBEDDING_FILE = '/usr/target/notebook/input/gfmodel_v4.vec'\n",
    "\n",
    "# embeddings_index = {}\n",
    "# f = open(EMBEDDING_FILE)\n",
    "# for line in tqdm(f):\n",
    "#     values = line.split(\" \")\n",
    "#     if len(values) == 302:\n",
    "#         word = values[0]\n",
    "#         coefs = np.asarray(values[1:301], dtype='float32')\n",
    "#         embeddings_index[word] = coefs\n",
    "# f.close()\n",
    "\n",
    "# print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding\n",
    "EMBEDDING_FILE = resources_dir/'cc.de.300.vec'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(EMBEDDING_FILE)\n",
    "for line in tqdm(f):\n",
    "    values = line.split(\" \")\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## embedding setup\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://)- number of memory cells set to less than 80-400 based on ([link](https://ai.stackexchange.com/questions/3156/how-to-select-number-of-hidden-layers-and-number-of-memory-cells-in-an-lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_init(kind = \"cpu\"):\n",
    "    model = Sequential()\n",
    "    if kind == \"gpu\":  \n",
    "        model.add(Embedding(max_features, embedding_vecor_length, input_length=maxlen,  weights=[embedding_matrix]))\n",
    "    else:\n",
    "        model.add(Embedding(max_features, embedding_vecor_length, input_length=maxlen,  weights=[embedding_matrix]))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    if kind == \"gpu\":  \n",
    "        model.add(CuDNNLSTM(128))\n",
    "    else:\n",
    "        model.add(LSTM(128))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {1: 1., 0: 1.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_init(kind)\n",
    "history = model.fit(train_X, train_y, epochs=10, batch_size=64, validation_data=(val_X, val_y)\n",
    "                    , callbacks=[EarlyStopping(monitor='val_loss', patience=2, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['acc'], label='train')\n",
    "plt.plot(history.history['val_acc'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots suggest that the model has a little over fitting problem, more data may help, but more epochs will not help using the current data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the extension and start TensorBoard\n",
    "\n",
    "# %load_ext tensorboard.notebook\n",
    "# %tensorboard --logdir logs\n",
    "# %reload_ext tensorboard.notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 3\n",
    "\n",
    "model = model_init()\n",
    "history = model.fit(train_X, train_y, epochs=epoch_num, batch_size=64, validation_data=(val_X, val_y)\n",
    "                    , callbacks=[EarlyStopping(monitor='val_loss', patience=2, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(test_X, test_y, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(test_X)\n",
    "labels = [x for x in test_y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall = precision_recall_curve(labels, prediction)\n",
    "precision_recall = pd.DataFrame.from_records(precision_recall).T\n",
    "precision_recall.columns = ['Precision', 'Recall', 'Thresholds']\n",
    "precision_recall[\"f1_score\"] = 2 * (precision_recall.Precision * precision_recall.Recall) / (precision_recall.Precision + precision_recall.Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model by max. f1-score\n",
    "print(\"Max f1-Score: %.2f%%\" % (max(precision_recall[\"f1_score\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find threshold for precision \n",
    "with open(cwd/\"output/qc_contact_request_deletion_reason_thresholds.txt\",\"w\") as file_thresholds:\n",
    "    precisions_thresholds = [0.8, 0.85, 0.9, 0.95] \n",
    "    for p in precisions_thresholds: \n",
    "        t = min(precision_recall[precision_recall['Precision']>p][\"Thresholds\"])\n",
    "        msg = \"Threshold with precision > %.2f%%: %.2f%%\" % (p, t)\n",
    "        print(msg)\n",
    "        file_thresholds.write(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print precision, recall, f1-score curve\n",
    "# # precision_recall.iplot(x='Thresholds', xTitle='Decision Threshold',yTitle='Score', title='Precision/Recall Tradeoff')\n",
    "# precision_recall.plot(x='Thresholds')\n",
    "\n",
    "# Trace1 can be viewed like a geom_point() layer with various arguements\n",
    "trace1 = go.Scatter(x=precision_recall.Thresholds, y = precision_recall.Precision,  marker=dict(size=5,\n",
    "                line=dict(width=1),\n",
    "                color=\"blue\"\n",
    "               ), \n",
    "                    mode=\"lines\", name='Precision')\n",
    "trace2 = go.Scatter(x=precision_recall.Thresholds, y = precision_recall.Recall,  marker=dict(size=5,\n",
    "                line=dict(width=1),\n",
    "                color=\"orange\"\n",
    "               ), \n",
    "                    mode=\"lines\", name='Recall')\n",
    "\n",
    "trace3 = go.Scatter(x=precision_recall.Thresholds, y = precision_recall.f1_score,  marker=dict(size=5,\n",
    "                line=dict(width=1),\n",
    "                color=\"green\"\n",
    "               ), \n",
    "                    mode=\"lines\", name='f1_score')\n",
    "\n",
    "\n",
    "\n",
    "data1 = go.Data([trace1, trace2, trace3])\n",
    "layout1=go.Layout(title=\"Threshold vs P, R, F1\", xaxis={'title':'Threshold'}, yaxis={'title':'Threshold Tradeoff'})\n",
    "figure1=go.Figure(data=data1,layout=layout1)\n",
    "iplot(figure1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predLabel = pd.DataFrame(prediction, columns = [\"Probability\"]).assign(Label=labels)\n",
    "trace4 = go.Histogram(\n",
    "    x=predLabel[predLabel.Label == 0].Probability,\n",
    "    opacity=1,\n",
    "    name = \"1\"\n",
    ")\n",
    "trace5 = go.Histogram(\n",
    "    x=predLabel[predLabel.Label == 1].Probability,\n",
    "    opacity=0.3,\n",
    "    name = \"0\"\n",
    ")\n",
    "\n",
    "data45 = go.Data([trace4, trace5])\n",
    "layout45 = go.Layout(barmode='overlay')\n",
    "figure45 = go.Figure(data=data45, layout=layout45)\n",
    "\n",
    "iplot(figure45, filename='probability overlaid histogram')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.9\n",
    "y_true = [x for x in test_y]\n",
    "y_pred = [1 if x > threshold else 0 for x in prediction]\n",
    "confusion_matrix(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionDF = pd.DataFrame({'prediction':prediction[:,0]})\n",
    "predicted_test = test.join(predictionDF, how='inner')\n",
    "predicted_test['y_pred'] = predicted_test['prediction'].progress_apply(lambda x: 1 if x > threshold else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false negatives\n",
    "predicted_test[(predicted_test['y_pred'] == 0) & (predicted_test['target'] == 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false positive\n",
    "predicted_test[(predicted_test['y_pred'] == 1) & (predicted_test['target'] == 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "modelfile = cwd/\"output/qc_contactrequest_reason_model.h5\"\n",
    "model.save(modelfile)\n",
    "\n",
    "# save tokenizer for model\n",
    "with open(cwd/'output/qc_contactrequest_reason_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(cwd/'output/qc_contactrequest_reason_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(tokenizer.to_json())    \n",
    "\n",
    "# save meta parameters\n",
    "with open(cwd/'output/qc_contactrequest_reason_meta_parameters.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(meta_parameters, f) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
