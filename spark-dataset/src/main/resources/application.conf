hdfs.host = "hdfs://gf-prod-01"
build.number = ${?BUILD_NUMBER}
organization = "net.gutefrage.data.ml"


spark {
  io.compression.codec = snappy
  rdd.compress = true
  serializer = org.apache.spark.serializer.KryoSerializer
  # avoid huge logs in bulk mode
  eventLog.enabled = false
  eventLog.dir = ${hdfs.host}/tmp
}

app.port = 4053

job {

  dwh.mysql = ${hdfs.host}/dwh/mysql/gutefrage4

  dwh2positive {
    spark {
      executor.memory = 6g
      executor.instances = 4
      executor.cores = 4
      yarn.executor.memoryOverhead = 10000
      yarn.driver.memoryOverhead = 2046
      # increase network timeout due to ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 160280 ms
      network.timeout = 10000000
      # if specified, ONLY THESE REASONS ARE DUMPED. Use full notation: question.contact-request

    }
    export.only = "question.contact-request"
    target = ${hdfs.host}/ivy-repo/releases/${organization}/
  }

  dwh2negative {
    spark {
      executor.memory = 6g
      executor.instances = 4
      executor.cores = 4
      yarn.executor.memoryOverhead = 10000
      yarn.driver.memoryOverhead = 2046
      # increase network timeout due to ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 160280 ms
      network.timeout = 10000000
      # if specified, ONLY THESE REASONS ARE DUMPED. Use full notation: question.contact-request

    }
    export.only = "question.notdeleted"
    target = ${hdfs.host}/ivy-repo/releases/${organization}/
  }
}

mysql {
  read {
    url = "jdbc:mysql://luggerwrite.endor.gutefrage.net:3306?zeroDateTimeBehavior=convertToNull&tinyInt1isBit=false"
    user = "rw_etl_job"
    password = "vvBZa98vaKP2"
  }
  write {
    url = "jdbc:mysql://luggerwrite.endor.gutefrage.net:3306"
    user = "rw_etl_job"
    password = "vvBZa98vaKP2"
  }
  stat.database = "experimental"
  stat.table = "ai_dataset_exp"
}