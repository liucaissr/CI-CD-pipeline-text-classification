hdfs.host = "hdfs://gf-prod-01"
build.number = ["0", ${?BUILD_NUMBER}]

spark {
  io.compression.codec = snappy
  rdd.compress = true
  serializer = org.apache.spark.serializer.KryoSerializer
  # avoid huge logs in bulk mode
  eventLog.enabled = false
  eventLog.dir = ${hdfs.host}/tmp
}


app.port = 4053

job {

  dwh.mysql = ${hdfs.host}/dwh/mysql/gutefrage4

  dwh2dataset {
    spark {
      executor.memory = 3g
      executor.instances = 2
      executor.cores = 4
      yarn.executor.memoryOverhead = 4096
      yarn.driver.memoryOverhead = 1024
      // increase network timeout due to ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 160280 ms
      network.timeout = 10000000
    }

    # if specified, ONLY THESE REASONS ARE DUMPED. Use full notation: question.contact-request
    export.only = "contact-request"

    target = ${hdfs.host}/data-projects/dataset/ivy-repo/releases/net.gutefrage.data/



  }
}
